=> creating model 'resnet18'
DataParallel will divide and allocate batch_size to all available GPUs
DataParallel will divide and allocate batch_size to all available GPUs
torch/nn/parallel/data_parallel.py: device_ids: [0, 1]
Time in DataParallel is 0.016574382781982422.
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.007195711135864258
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 1.7756280899047852
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.6683731079101562
Time spent in bn1 is: 0.005536317825317383
Time spent in relu is: 0.0017719268798828125
Time spent in maxpool is: 0.02328801155090332
Time spent in A is: 0.6989693641662598
Time spent in conv1 is: 0.9423236846923828
Time spent in B is: 0.24419426918029785
Time spent in C is: 0.0013904571533203125
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.9447340965270996
Time spent in bn1 is: 0.005399465560913086
Time spent in relu is: 0.0016617774963378906
Time spent in maxpool is: 0.0034193992614746094
Time spent in A is: 0.9528043270111084
Time spent in B is: 0.2397899627685547
Time spent in C is: 0.0002372264862060547
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  1.1930124759674072
Time spent in model() is 3.2134549617767334
Before going into criterion
Time spent in criterion is 0.004153013229370117
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00026035308837890625
Time spent in the rest is 0.509584903717041
Batch 0
Data wait time is 2.375786304473877.
Transfer time is 0.0003616809844970703.
Compute time is 3.72739315032959.
Total data wait time is: 2.375786304473877
Total compute time is: 3.72739315032959
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006264925003051758
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006546735763549805
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013363838195800781
Time spent in conv1 is: 0.014133453369140625
Time spent in bn1 is: 0.004099369049072266
Time spent in bn1 is: 0.004403591156005859
Time spent in relu is: 0.001604318618774414
Time spent in relu is: 0.0016453266143798828
Time spent in maxpool is: 0.002163410186767578
Time spent in A is: 0.022000551223754883
Time spent in maxpool is: 0.002248525619506836
Time spent in A is: 0.02166128158569336
Time spent in B is: 0.2323601245880127
Time spent in C is: 0.0004284381866455078
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2549872398376465
Time spent in B is: 0.23293399810791016
Time spent in C is: 0.00042247772216796875
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25525903701782227
Time spent in model() is 0.26995086669921875
Before going into criterion
Time spent in criterion is 0.00044918060302734375
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002739429473876953
Time spent in the rest is 0.5053770542144775
Batch 1
Data wait time is 0.00032210350036621094.
Transfer time is 0.0002613067626953125.
Compute time is 0.7758996486663818.
Total data wait time is: 2.376108407974243
Total compute time is: 4.503292798995972
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006277799606323242
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006575345993041992
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012698650360107422
Time spent in conv1 is: 0.013252496719360352
Time spent in bn1 is: 0.004071474075317383
Time spent in bn1 is: 0.004103422164916992
Time spent in relu is: 0.0016491413116455078
Time spent in relu is: 0.0016560554504394531
Time spent in maxpool is: 0.0022110939025878906
Time spent in A is: 0.020630359649658203
Time spent in maxpool is: 0.0022592544555664062
Time spent in A is: 0.021271228790283203
Time spent in B is: 0.23263263702392578
Time spent in B is: 0.2326352596282959
Time spent in C is: 0.0004448890686035156
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25386834144592285
Time spent in C is: 0.00058746337890625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2546865940093994
Time spent in model() is 0.2684011459350586
Before going into criterion
Time spent in criterion is 0.00046133995056152344
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.000301361083984375
Time spent in the rest is 0.5066187381744385
Batch 2
Data wait time is 0.00028252601623535156.
Transfer time is 0.00031280517578125.
Compute time is 0.7756366729736328.
Total data wait time is: 2.3763909339904785
Total compute time is: 5.2789294719696045
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006269931793212891
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006541013717651367
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013278484344482422
Time spent in conv1 is: 0.012937545776367188
Time spent in bn1 is: 0.004179716110229492
Time spent in bn1 is: 0.0042421817779541016
Time spent in relu is: 0.0016756057739257812
Time spent in relu is: 0.0016443729400634766
Time spent in maxpool is: 0.0022416114807128906
Time spent in A is: 0.021375417709350586
Time spent in maxpool is: 0.002289295196533203
Time spent in A is: 0.02111339569091797
Time spent in B is: 0.2322835922241211
Time spent in C is: 0.0003082752227783203
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25417089462280273
Time spent in B is: 0.23272252082824707
Time spent in C is: 0.0002498626708984375
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25423312187194824
Time spent in model() is 0.26828455924987793
Before going into criterion
Time spent in criterion is 0.000301361083984375
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002448558807373047
Time spent in the rest is 0.5051660537719727
Batch 3
Data wait time is 0.00025916099548339844.
Transfer time is 0.0002906322479248047.
Compute time is 0.7738602161407471.
Total data wait time is: 2.376650094985962
Total compute time is: 6.052789688110352
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006335020065307617
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006596565246582031
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012745857238769531
Time spent in conv1 is: 0.013412952423095703
Time spent in bn1 is: 0.004083395004272461
Time spent in bn1 is: 0.004100799560546875
Time spent in relu is: 0.0016541481018066406
Time spent in relu is: 0.0016510486602783203
Time spent in maxpool is: 0.00220489501953125
Time spent in A is: 0.020688295364379883
Time spent in maxpool is: 0.0022668838500976562
Time spent in A is: 0.021431684494018555
Time spent in B is: 0.23224782943725586
Time spent in B is: 0.23262619972229004
Time spent in C is: 0.0004200935363769531
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25426363945007324
Time spent in C is: 0.000637054443359375
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2541680335998535
Time spent in model() is 0.26817870140075684
Before going into criterion
Time spent in criterion is 0.0002903938293457031
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002930164337158203
Time spent in the rest is 0.5058224201202393
Batch 4
Data wait time is 0.00026035308837890625.
Transfer time is 0.0002465248107910156.
Compute time is 0.7744095325469971.
Total data wait time is: 2.376910448074341
Total compute time is: 6.827199220657349
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0062901973724365234
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006600856781005859
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.01271677017211914
Time spent in conv1 is: 0.013397455215454102
Time spent in bn1 is: 0.004225969314575195
Time spent in bn1 is: 0.004229307174682617
Time spent in relu is: 0.001653432846069336
Time spent in relu is: 0.0016434192657470703
Time spent in maxpool is: 0.0022008419036865234
Time spent in A is: 0.020797014236450195
Time spent in maxpool is: 0.0022721290588378906
Time spent in A is: 0.02154231071472168
Time spent in B is: 0.23221921920776367
Time spent in B is: 0.23261570930480957
Time spent in C is: 0.00043845176696777344
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25437331199645996
Time spent in C is: 0.0003905296325683594
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.254016637802124
Time spent in model() is 0.26825833320617676
Before going into criterion
Time spent in criterion is 0.0004730224609375
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002944469451904297
Time spent in the rest is 0.5058379173278809
Batch 5
Data wait time is 0.00047278404235839844.
Transfer time is 0.00010538101196289062.
Compute time is 0.7747371196746826.
Total data wait time is: 2.377383232116699
Total compute time is: 7.601936340332031
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006296634674072266
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006610393524169922
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012746810913085938
Time spent in conv1 is: 0.013488292694091797
Time spent in bn1 is: 0.0042874813079833984
Time spent in bn1 is: 0.00428462028503418
Time spent in relu is: 0.0016894340515136719
Time spent in relu is: 0.0016889572143554688
Time spent in maxpool is: 0.002244710922241211
Time spent in A is: 0.02096843719482422
Time spent in maxpool is: 0.0022809505462646484
Time spent in A is: 0.021742820739746094
Time spent in B is: 0.23253607749938965
Time spent in B is: 0.23291540145874023
Time spent in C is: 0.0002949237823486328
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2543492317199707
Time spent in C is: 0.0006341934204101562
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2551150321960449
Time spent in model() is 0.26903676986694336
Before going into criterion
Time spent in criterion is 0.0005135536193847656
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002970695495605469
Time spent in the rest is 0.5091116428375244
Batch 6
Data wait time is 0.00037860870361328125.
Transfer time is 0.000469207763671875.
Compute time is 0.778876543045044.
Total data wait time is: 2.3777618408203125
Total compute time is: 8.380812883377075
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006542205810546875
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006926059722900391
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012811899185180664
Time spent in conv1 is: 0.013550996780395508
Time spent in bn1 is: 0.0040433406829833984
Time spent in bn1 is: 0.004454135894775391
Time spent in relu is: 0.0016655921936035156
Time spent in relu is: 0.0016627311706542969
Time spent in maxpool is: 0.0021944046020507812
Time spent in A is: 0.021454334259033203
Time spent in maxpool is: 0.0022232532501220703
Time spent in A is: 0.021152019500732422
Time spent in B is: 0.2326641082763672
Time spent in B is: 0.23333501815795898
Time spent in C is: 0.0010471343994140625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2554202079772949
Time spent in C is: 0.0007197856903076172
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2554285526275635
Time spent in model() is 0.27017927169799805
Before going into criterion
Time spent in criterion is 0.0004417896270751953
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00023674964904785156
Time spent in the rest is 0.5059351921081543
Batch 7
Data wait time is 0.0003733634948730469.
Transfer time is 0.00037026405334472656.
Compute time is 0.7766897678375244.
Total data wait time is: 2.3781352043151855
Total compute time is: 9.1575026512146
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006302833557128906
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006598711013793945
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.01270604133605957
Time spent in conv1 is: 0.013316631317138672
Time spent in bn1 is: 0.004060029983520508
Time spent in bn1 is: 0.004091024398803711
Time spent in relu is: 0.00165557861328125
Time spent in relu is: 0.0017096996307373047
Time spent in maxpool is: 0.0022478103637695312
Time spent in A is: 0.02066946029663086
Time spent in maxpool is: 0.002249479293823242
Time spent in A is: 0.02136683464050293
Time spent in B is: 0.2327289581298828
Time spent in B is: 0.23279190063476562
Time spent in C is: 0.0005037784576416016
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2540853023529053
Time spent in C is: 0.00041604042053222656
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2547447681427002
Time spent in model() is 0.2684764862060547
Before going into criterion
Time spent in criterion is 0.0004966259002685547
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00024628639221191406
Time spent in the rest is 0.5063376426696777
Batch 8
Data wait time is 0.0003197193145751953.
Transfer time is 0.00030517578125.
Compute time is 0.7754714488983154.
Total data wait time is: 2.3784549236297607
Total compute time is: 9.932974100112915
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006313800811767578
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006595134735107422
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012722015380859375
Time spent in conv1 is: 0.013421058654785156
Time spent in bn1 is: 0.004023551940917969
Time spent in bn1 is: 0.004072904586791992
Time spent in relu is: 0.0016529560089111328
Time spent in relu is: 0.0016794204711914062
Time spent in maxpool is: 0.0021944046020507812
Time spent in A is: 0.020592927932739258
Time spent in maxpool is: 0.002238750457763672
Time spent in A is: 0.021412134170532227
Time spent in B is: 0.23283004760742188
Time spent in C is: 0.000370025634765625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25402283668518066
Time spent in B is: 0.23360180854797363
Time spent in C is: 0.0005357265472412109
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2557704448699951
Time spent in model() is 0.269512414932251
Before going into criterion
Time spent in criterion is 0.00040435791015625
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00028514862060546875
Time spent in the rest is 0.5054965019226074
Batch 9
Data wait time is 0.0002808570861816406.
Transfer time is 0.0002734661102294922.
Compute time is 0.7755715847015381.
Total data wait time is: 2.3787357807159424
Total compute time is: 10.708545684814453
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006268978118896484
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006589651107788086
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012788057327270508
Time spent in conv1 is: 0.013609170913696289
Time spent in bn1 is: 0.003965139389038086
Time spent in bn1 is: 0.004264354705810547
Time spent in relu is: 0.0016236305236816406
Time spent in relu is: 0.0016505718231201172
Time spent in maxpool is: 0.0021855831146240234
Time spent in A is: 0.02138352394104004
Time spent in maxpool is: 0.002268075942993164
Time spent in A is: 0.020971059799194336
Time spent in B is: 0.23233532905578613
Time spent in C is: 0.0004303455352783203
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25440263748168945
Time spent in B is: 0.233001708984375
Time spent in C is: 0.0003943443298339844
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2545795440673828
Time spent in model() is 0.2692537307739258
Before going into criterion
Time spent in criterion is 0.0005583763122558594
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00032520294189453125
Time spent in the rest is 0.5067942142486572
Batch 10
Data wait time is 0.0004849433898925781.
Transfer time is 0.0004570484161376953.
Compute time is 0.7767877578735352.
Total data wait time is: 2.379220724105835
Total compute time is: 11.485333442687988
