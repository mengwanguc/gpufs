=> creating model 'resnet18'
DataParallel will divide and allocate batch_size to all available GPUs
DataParallel will divide and allocate batch_size to all available GPUs
torch/nn/parallel/data_parallel.py: device_ids: [0, 1]
Time in DataParallel is 0.01644611358642578.
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.007048130035400391
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 1.6070687770843506
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.5942323207855225
Time spent in bn1 is: 0.004937648773193359
Time spent in relu is: 0.001706838607788086
Time spent in maxpool is: 0.0030786991119384766
Time spent in A is: 0.6039555072784424
Time spent in conv1 is: 0.8440749645233154
Time spent in B is: 0.24191665649414062
Time spent in C is: 0.0017390251159667969
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.8477969169616699
Time spent in bn1 is: 0.008110523223876953
Time spent in relu is: 0.0017120838165283203
Time spent in maxpool is: 0.0037696361541748047
Time spent in A is: 0.8576672077178955
Time spent in B is: 0.24670839309692383
Time spent in C is: 0.00029850006103515625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  1.1048064231872559
Time spent in model() is 2.9536163806915283
Before going into criterion
Time spent in criterion is 0.003744363784790039
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00023174285888671875
Time spent in the rest is 0.5079069137573242
Batch 0
Data wait time is 2.9396491050720215.
Transfer time is 0.0003654956817626953.
Compute time is 3.46545147895813.
Total data wait time is: 2.9396491050720215
Total compute time is: 3.46545147895813
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006327152252197266
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006609439849853516
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013674736022949219
Time spent in conv1 is: 0.014278173446655273
Time spent in bn1 is: 0.0040607452392578125
Time spent in bn1 is: 0.004071474075317383
Time spent in relu is: 0.0016391277313232422
Time spent in relu is: 0.0016460418701171875
Time spent in maxpool is: 0.0022068023681640625
Time spent in A is: 0.021581411361694336
Time spent in maxpool is: 0.002257108688354492
Time spent in A is: 0.022252798080444336
Time spent in B is: 0.23290205001831055
Time spent in B is: 0.23293232917785645
Time spent in C is: 0.00045228004455566406
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2551095485687256
Time spent in C is: 0.0005586147308349609
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2559356689453125
Time spent in model() is 0.2700190544128418
Before going into criterion
Time spent in criterion is 0.0007338523864746094
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00015544891357421875
Time spent in the rest is 0.5053770542144775
Batch 1
Data wait time is 0.00020647048950195312.
Transfer time is 0.0002372264862060547.
Compute time is 0.7765905857086182.
Total data wait time is: 2.9398555755615234
Total compute time is: 4.242042064666748
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006507158279418945
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.007050752639770508
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012792587280273438
Time spent in conv1 is: 0.013793230056762695
Time spent in bn1 is: 0.0040776729583740234
Time spent in bn1 is: 0.004540205001831055
Time spent in relu is: 0.0016548633575439453
Time spent in relu is: 0.0021338462829589844
Time spent in maxpool is: 0.0023801326751708984
Time spent in A is: 0.021367788314819336
Time spent in maxpool is: 0.002366304397583008
Time spent in A is: 0.02237105369567871
Time spent in B is: 0.23349380493164062
Time spent in B is: 0.23361992835998535
Time spent in C is: 0.0004932880401611328
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2555406093597412
Time spent in C is: 0.0005815029144287109
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25679588317871094
Time spent in model() is 0.27213311195373535
Before going into criterion
Time spent in criterion is 0.0006582736968994141
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002276897430419922
Time spent in the rest is 0.5054781436920166
Batch 2
Data wait time is 0.0007870197296142578.
Transfer time is 0.0007321834564208984.
Compute time is 0.7784724235534668.
Total data wait time is: 2.9406425952911377
Total compute time is: 5.020514488220215
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006437778472900391
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006704807281494141
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013471126556396484
Time spent in conv1 is: 0.012970209121704102
Time spent in bn1 is: 0.004024982452392578
Time spent in bn1 is: 0.0040547847747802734
Time spent in relu is: 0.0016412734985351562
Time spent in relu is: 0.001682281494140625
Time spent in maxpool is: 0.002212047576904297
Time spent in A is: 0.021349430084228516
Time spent in maxpool is: 0.002276182174682617
Time spent in A is: 0.020983457565307617
Time spent in B is: 0.23357582092285156
Time spent in B is: 0.2336750030517578
Time spent in C is: 0.0005297660827636719
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2556765079498291
Time spent in C is: 0.0006313323974609375
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2555079460144043
Time spent in model() is 0.2701444625854492
Before going into criterion
Time spent in criterion is 0.00032067298889160156
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00020074844360351562
Time spent in the rest is 0.5035994052886963
Batch 3
Data wait time is 0.0002846717834472656.
Transfer time is 0.0002777576446533203.
Compute time is 0.7742092609405518.
Total data wait time is: 2.940927267074585
Total compute time is: 5.794723749160767
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006348609924316406
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.0066716670989990234
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013354063034057617
Time spent in conv1 is: 0.012947559356689453
Time spent in bn1 is: 0.003936767578125
Time spent in bn1 is: 0.004194974899291992
Time spent in relu is: 0.0016124248504638672
Time spent in relu is: 0.0016269683837890625
Time spent in maxpool is: 0.0021665096282958984
Time spent in A is: 0.02066326141357422
Time spent in maxpool is: 0.0022017955780029297
Time spent in A is: 0.0213778018951416
Time spent in B is: 0.23299145698547363
Time spent in C is: 0.00047898292541503906
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25435590744018555
Time spent in B is: 0.23372292518615723
Time spent in C is: 0.0004353523254394531
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2558145523071289
Time spent in model() is 0.26967740058898926
Before going into criterion
Time spent in criterion is 0.0004999637603759766
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002033710479736328
Time spent in the rest is 0.5048682689666748
Batch 4
Data wait time is 0.00032520294189453125.
Transfer time is 0.00033402442932128906.
Compute time is 0.7752671241760254.
Total data wait time is: 2.9412524700164795
Total compute time is: 6.569990873336792
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006372928619384766
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.00677800178527832
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013429880142211914
Time spent in conv1 is: 0.012994766235351562
Time spent in bn1 is: 0.004178285598754883
Time spent in bn1 is: 0.0042073726654052734
Time spent in relu is: 0.0016469955444335938
Time spent in relu is: 0.0016627311706542969
Time spent in maxpool is: 0.002207517623901367
Time spent in A is: 0.021462678909301758
Time spent in maxpool is: 0.0022547245025634766
Time spent in A is: 0.02111959457397461
Time spent in B is: 0.23314857482910156
Time spent in B is: 0.233154296875
Time spent in C is: 0.000446319580078125
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25528860092163086
Time spent in C is: 0.0004031658172607422
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25485682487487793
Time spent in model() is 0.26983022689819336
Before going into criterion
Time spent in criterion is 0.0005269050598144531
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00019168853759765625
Time spent in the rest is 0.5054836273193359
Batch 5
Data wait time is 0.0002789497375488281.
Transfer time is 0.0003414154052734375.
Compute time is 0.7760567665100098.
Total data wait time is: 2.9415314197540283
Total compute time is: 7.346047639846802
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006311178207397461
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006640195846557617
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013477325439453125
Time spent in conv1 is: 0.01311349868774414
Time spent in bn1 is: 0.00427556037902832
Time spent in bn1 is: 0.004281282424926758
Time spent in relu is: 0.0016832351684570312
Time spent in relu is: 0.0016863346099853516
Time spent in maxpool is: 0.0022630691528320312
Time spent in A is: 0.021699190139770508
Time spent in maxpool is: 0.002376556396484375
Time spent in A is: 0.021457672119140625
Time spent in B is: 0.23329734802246094
Time spent in B is: 0.23374700546264648
Time spent in C is: 0.00031638145446777344
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2560265064239502
Time spent in C is: 0.0007839202880859375
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2557835578918457
Time spent in model() is 0.27040886878967285
Before going into criterion
Time spent in criterion is 0.0005402565002441406
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00021457672119140625
Time spent in the rest is 0.5049726963043213
Batch 6
Data wait time is 0.00031566619873046875.
Transfer time is 0.00030231475830078125.
Compute time is 0.7761571407318115.
Total data wait time is: 2.941847085952759
Total compute time is: 8.122204780578613
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0063059329986572266
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006600141525268555
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.013487100601196289
Time spent in conv1 is: 0.013041019439697266
Time spent in bn1 is: 0.004076480865478516
Time spent in bn1 is: 0.004070281982421875
Time spent in relu is: 0.0016336441040039062
Time spent in relu is: 0.001644134521484375
Time spent in maxpool is: 0.002192258834838867
Time spent in A is: 0.021389484405517578
Time spent in maxpool is: 0.0022301673889160156
Time spent in A is: 0.02098560333251953
Time spent in B is: 0.23297739028930664
Time spent in B is: 0.23299217224121094
Time spent in C is: 0.0003693103790283203
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2545316219329834
Time spent in C is: 0.0007419586181640625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2553868293762207
Time spent in model() is 0.26951122283935547
Before going into criterion
Time spent in criterion is 0.0005443096160888672
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.00019669532775878906
Time spent in the rest is 0.5052740573883057
Batch 7
Data wait time is 0.0003752708435058594.
Transfer time is 0.00039124488830566406.
Compute time is 0.7754919528961182.
Total data wait time is: 2.9422223567962646
Total compute time is: 8.897696733474731
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006529808044433594
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006815433502197266
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012730836868286133
Time spent in conv1 is: 0.013433694839477539
Time spent in bn1 is: 0.00409245491027832
Time spent in bn1 is: 0.004178762435913086
Time spent in relu is: 0.0016715526580810547
Time spent in relu is: 0.0016431808471679688
Time spent in maxpool is: 0.002233743667602539
Time spent in A is: 0.020728588104248047
Time spent in maxpool is: 0.0022733211517333984
Time spent in A is: 0.021528959274291992
Time spent in B is: 0.23298978805541992
Time spent in B is: 0.2330164909362793
Time spent in C is: 0.0004901885986328125
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25442075729370117
Time spent in C is: 0.000640869140625
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2553887367248535
Time spent in model() is 0.2694427967071533
Before going into criterion
Time spent in criterion is 0.0005161762237548828
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.000194549560546875
Time spent in the rest is 0.5051624774932861
Batch 8
Data wait time is 0.00047016143798828125.
Transfer time is 0.00012445449829101562.
Compute time is 0.775341272354126.
Total data wait time is: 2.942692518234253
Total compute time is: 9.673038005828857
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006441831588745117
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006840944290161133
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012741565704345703
Time spent in conv1 is: 0.013535499572753906
Time spent in bn1 is: 0.004252195358276367
Time spent in bn1 is: 0.00431060791015625
Time spent in relu is: 0.0017170906066894531
Time spent in relu is: 0.0016760826110839844
Time spent in maxpool is: 0.002239704132080078
Time spent in A is: 0.0209505558013916
Time spent in maxpool is: 0.002317190170288086
Time spent in A is: 0.021839380264282227
Time spent in B is: 0.2335669994354248
Time spent in B is: 0.23372697830200195
Time spent in C is: 0.0005393028259277344
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2552676200866699
Time spent in C is: 0.0004439353942871094
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2562370300292969
Time spent in model() is 0.270580530166626
Before going into criterion
Time spent in criterion is 0.000553131103515625
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.000194549560546875
Time spent in the rest is 0.5053501129150391
Batch 9
Data wait time is 0.0008752346038818359.
Transfer time is 0.00018668174743652344.
Compute time is 0.7766861915588379.
Total data wait time is: 2.9435677528381348
Total compute time is: 10.449724197387695
args.gpu is None. I might put images onto GPU later in model. 
The image is on cuda before model: False
Before going into model()
I got into forward funciton of DataParallel! ! !, the ids are [0, 1]
Note that the images are not divided here yet. 
The length of inputs is 1, and the length of kwargs is 0
Check if the image is on CUDA: False
I got into function scatter in data_parallel.py. 
Got into scatter_kwargs. 
Got into try. 
The length of input right now in scatter_map: 1
Got into isinstance(obj, tuple) and len(obj) > 0. 
The length of input right now in scatter_map: 256
Got into isinstance(obj, torch.Tensor). 
Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 3, 224, 224]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.006337642669677734
Got into finally. 
The length of the res is: 2
After scatter inputs. 
Length of inputs: 2, length of kwargs: 0
The total time spent on scatter is: 0.006641387939453125
After scatter
The length of inputs is 2, and the length of kwargs is 2
Two inputs, the first one is in CUDA: True
Two inputs, the second one is in CUDA: True
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
forward! ! ! ! ! ! ! ! ! ! 
_forward_impl! ! ! ! ! ! ! ! ! ! 
Got into _forward_impl in resnet.py.
Time spent in conv1 is: 0.012748479843139648
Time spent in conv1 is: 0.013401269912719727
Time spent in bn1 is: 0.003942966461181641
Time spent in bn1 is: 0.00428009033203125
Time spent in relu is: 0.0016231536865234375
Time spent in relu is: 0.0016622543334960938
Time spent in maxpool is: 0.0042684078216552734
Time spent in A is: 0.022959232330322266
Time spent in B is: 0.23325037956237793
Time spent in C is: 0.0004410743713378906
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.25510096549987793
Time spent in B is: 0.23282217979431152
Time spent in C is: 0.00040841102600097656
Getting out of _forward_impl in resnet.py
Time spent in _forward_impl was  0.2564210891723633
Time spent in model() is 0.2708089351654053
Before going into criterion
Time spent in criterion is 0.0004482269287109375
Before going into the rest
-- call stack of Function forward in backward
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 89, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore
File "/home/cc/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 79, in backward
    for line in traceback.format_stack():
---

Got into _functions.py Scatter's forward
The length of tensor is: torch.Size([256, 1000]).
out is None. So I'm here. the devices are: [0, 1]
The time spent in _C_scatter is: 0.0002448558807373047
Time spent in the rest is 0.5039854049682617
Batch 10
Data wait time is 0.0003821849822998047.
Transfer time is 0.0002689361572265625.
Compute time is 0.7754290103912354.
Total data wait time is: 2.9439499378204346
Total compute time is: 11.22515320777893
0…Ô‘¡U                       0       Q        ¸æJØ  Ä∫Ó‘¡U          0       `œ‚«¡U  P