{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNStall Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./rtx6000/\"\n",
    "batch_size = 256\n",
    "epoch_size = 2560\n",
    "batches_per_epoch = epoch_size // batch_size\n",
    "\n",
    "runs = {}\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    path = folder + filename\n",
    "    \n",
    "    name = filename.split('-')\n",
    "    model = name[0]\n",
    "    cores = int(name[2])\n",
    "    if not model in runs:\n",
    "        runs[model] = []\n",
    "    \n",
    "    epoch_time = pd.read_csv(path, '\\t').sum().sum()\n",
    "    samples_per_sec = epoch_size / epoch_time\n",
    "    \n",
    "    runs[model].append((cores, samples_per_sec))\n",
    "\n",
    "for model, data in runs.items():\n",
    "    x, y = zip(*sorted(data))\n",
    "    plt.plot(x, y, label = model, marker='o')\n",
    "    \n",
    "plt.title(\"RTX6000 Pre-process Cores vs. Performance\")\n",
    "plt.ylabel(\"Samples/sec\")\n",
    "plt.xlabel(\"Cores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNStall Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/p100-resnet18/\"\n",
    "n_epochs = 1\n",
    "\n",
    "runs = []\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    path = folder + filename\n",
    "    \n",
    "    name = filename.split('-')\n",
    "    model = name[0]\n",
    "    batch_size = name[1]\n",
    "    workers = name[3]\n",
    "    memory = name[5]\n",
    "    cached = name[7]\n",
    "    \n",
    "    data = pd.read_csv(path, '\\t')\n",
    "    io_time = sum(data['io_time']) / n_epochs\n",
    "    cpu2gpu_time = sum(data['cpu2gpu_time']) / n_epochs\n",
    "    gpu_time = sum(data['gpu_time']) / n_epochs\n",
    "    \n",
    "    runs.append((float(name[7][:-1]), cached + '\\n(' + memory + ')', io_time, cpu2gpu_time, gpu_time))\n",
    "\n",
    "_, cached, io, cpu2gpu, gpu = zip(*sorted(runs))\n",
    "bars = np.arange(len(cached))\n",
    "plt.bar(cached, io, bottom=np.array(cpu2gpu) + np.array(gpu), label='io')\n",
    "plt.bar(cached, cpu2gpu, bottom=gpu, label='cpu2gpu')\n",
    "plt.bar(cached, gpu, label='gpu')\n",
    "plt.title(\"P100 Memory vs. Performance (resnet18)\")\n",
    "plt.ylabel(\"Epoch time (s)\")\n",
    "plt.xlabel(\"Cache %\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNStall Figure 6 (prep stall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/p100-fig6/\"\n",
    "n_epochs = 1\n",
    "\n",
    "prep_stall_pct = []\n",
    "prep_stall_abs = {}\n",
    "\n",
    "nicknames = {\n",
    "    \"alexnet\":\"AN\",\n",
    "    \"mobilenet_v2\":\"MN\",\n",
    "    \"resnet18\":\"RN18\",\n",
    "    \"shufflenet_v2_x0_5\":\"ShN\",\n",
    "    \"squeezenet1_0\":\"SqN\",\n",
    "    \"vgg11\":\"V11\",\n",
    "}\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    path = folder + filename\n",
    "    \n",
    "    name = filename.split('-')\n",
    "    model = nicknames[name[0]]\n",
    "    batch_size = name[1]\n",
    "    workers = name[3]\n",
    "    memory = name[5]\n",
    "    cached = name[7]\n",
    "    \n",
    "    data = pd.read_csv(path, '\\t')\n",
    "    io_time = sum(data['io_time']) / n_epochs\n",
    "    cpu2gpu_time = sum(data['cpu2gpu_time']) / n_epochs\n",
    "    gpu_time = sum(data['gpu_time']) / n_epochs\n",
    "    \n",
    "    prep_stall_pct.append((100 * io_time / (io_time + cpu2gpu_time + gpu_time), model))\n",
    "    prep_stall_abs[model] = io_time\n",
    "\n",
    "times, models = zip(*sorted(prep_stall_pct, reverse=True))\n",
    "\n",
    "plt.bar(models, times)\n",
    "plt.title(\"P100 Prep Stalls\")\n",
    "plt.ylabel(\"Prep Stall % of Epoch Time\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNStall Figure 3 (fetch stall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/p100-fig3/\"\n",
    "n_epochs = 1\n",
    "\n",
    "fetch_stall_pct = []\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    path = folder + filename\n",
    "    \n",
    "    name = filename.split('-')\n",
    "    model = nicknames[name[0]]\n",
    "    batch_size = name[1]\n",
    "    workers = name[3]\n",
    "    memory = name[5]\n",
    "    cached = name[7]\n",
    "    \n",
    "    data = pd.read_csv(path, '\\t')\n",
    "    io_time = sum(data['io_time']) / n_epochs\n",
    "    cpu2gpu_time = sum(data['cpu2gpu_time']) / n_epochs\n",
    "    gpu_time = sum(data['gpu_time']) / n_epochs\n",
    "    \n",
    "    fetch_stall_pct.append((100 * (io_time - prep_stall_abs[model]) / (io_time + cpu2gpu_time + gpu_time), model))\n",
    "\n",
    "nicknames = {\n",
    "    \"alexnet\":\"AN\",\n",
    "    \"mobilenet_v2\":\"MN\",\n",
    "    \"resnet18\":\"RN18\",\n",
    "    \"shufflenet_v2_x0_5\":\"ShN\",\n",
    "    \"squeezenet1_0\":\"SqN\",\n",
    "    \"vgg11\":\"V11\",\n",
    "}\n",
    "\n",
    "times, models = zip(*sorted(fetch_stall_pct, reverse=True))\n",
    "\n",
    "plt.bar(models, times)\n",
    "plt.title(\"P100 Fetch Stalls (~38% cached)\")\n",
    "plt.ylabel(\"Fetch Stall % of Epoch Time\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker/Mem Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/p100-worker-mem-alexnet/\"\n",
    "n_epochs = 1\n",
    "\n",
    "mem_cfgs = [\"4G\", \"6G\", \"8G\", \"10G\", \"12G\", \"14G\", \"16G\", \"18G\", \"20G\", \"22G\", \"24G\"]\n",
    "worker_cfgs = [\"1\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\"]\n",
    "\n",
    "out_cached = np.zeros((len(mem_cfgs), len(worker_cfgs)), dtype=float)\n",
    "out_timing = np.zeros((len(mem_cfgs), len(worker_cfgs)), dtype=float)\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    path = folder + filename\n",
    "    \n",
    "    name = filename.split('-')\n",
    "    model = name[0]\n",
    "    batch_size = name[1]\n",
    "    workers = name[3]\n",
    "    memory = name[5]\n",
    "    cached = name[7]\n",
    "    \n",
    "    data = pd.read_csv(path, '\\t')\n",
    "    io_time = sum(data['io_time']) / n_epochs\n",
    "    cpu2gpu_time = sum(data['cpu2gpu_time']) / n_epochs\n",
    "    gpu_time = sum(data['gpu_time']) / n_epochs\n",
    "    total_time = io_time + cpu2gpu_time + gpu_time\n",
    "    \n",
    "    out_cached[mem_cfgs.index(memory), worker_cfgs.index(workers)] = float(cached[:-1])\n",
    "    out_timing[mem_cfgs.index(memory), worker_cfgs.index(workers)] = io_time / total_time\n",
    "\n",
    "missing_data = [(4, 0), (5, 0), (6, 0), (5, 1), (6, 1)]\n",
    "for i, j in missing_data:\n",
    "    out_cached[j, i] = float('nan')\n",
    "    out_timing[j, i] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.get_cmap('RdYlGn')\n",
    "cmap.set_bad(color='gray')\n",
    "im = ax.imshow(out_cached, cmap=cmap)\n",
    "\n",
    "ax.set_yticks(np.arange(len(mem_cfgs)))\n",
    "ax.set_yticklabels(mem_cfgs)\n",
    "\n",
    "ax.set_xticks(np.arange(len(worker_cfgs)))\n",
    "ax.set_xticklabels(worker_cfgs)\n",
    "\n",
    "for i in range(len(mem_cfgs)):\n",
    "    for j in range(len(worker_cfgs)):\n",
    "        if not ((j, i) in missing_data):\n",
    "            text = ax.text(j, i, out_cached[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "        else:\n",
    "            text = ax.text(j, i, \"N/A\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "  \n",
    "plt.title(\"% Cached\")\n",
    "plt.xlabel(\"Workers\")\n",
    "plt.ylabel(\"Memory Limit\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.savefig(\"cached.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.get_cmap('RdYlGn').reversed()\n",
    "cmap.set_bad(color='gray')\n",
    "im = ax.imshow(out_timing, cmap=cmap)\n",
    "\n",
    "ax.set_yticks(np.arange(len(mem_cfgs)))\n",
    "ax.set_yticklabels(mem_cfgs)\n",
    "\n",
    "ax.set_xticks(np.arange(len(worker_cfgs)))\n",
    "ax.set_xticklabels(worker_cfgs)\n",
    "\n",
    "for i in range(len(mem_cfgs)):\n",
    "    for j in range(len(worker_cfgs)):\n",
    "        if not ((j, i) in missing_data):\n",
    "            text = ax.text(j, i, round(out_timing[i, j] * 100, 1), ha=\"center\", va=\"center\", color=\"black\")\n",
    "        else:\n",
    "            text = ax.text(j, i, \"N/A\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "  \n",
    "plt.title(\"Data Stall % of Epoch Time\")\n",
    "plt.xlabel(\"Workers\")\n",
    "plt.ylabel(\"Memory Limit\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.savefig(\"data_stall.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
